# -*- coding: utf-8 -*-
"""Braindetector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j4sKi73kMDGhMHfc-9f_Lfuh9U5c35lR

### Comparative Study of Conventional Machine Learning and Deep Learning Models in MRI Image-based Brain Tumor Detection

### Research Questions

1. What are the differences between conventional machine learning models and convolutional neural networks (CNNs) and transfer learning models while detecting brain tumors using MRI images?
2. Would enhanced Transfer Learning enhance perfomance when compaired to traditional and raw cnn models?
3. To what extent do the models match real labels, as indicated by kappa score and other metrics of evaluation?

### Working Approach
For the development of the project, My main goal will be to compaire the performance of various machine learning models under this reseach in detecting brain tumors from MRI images. The model categrories to be compaired will be:

- Traditional models i.e  Logistic Regression (linear model) and Decision Tree (non-linear tree based model)
- Deep learning like
    - Handcrafted Convolutional Neural Network (CNN)
    - Pre-trained transfer learning models
        - VGG16
        - ResNet50

I will perfom image processing like scaling, transfoming, histogram equalization etc with the main goal being to  improve feature visibility and classification efficiency. All models will be tested for accuracy, F1-score, and Cohen's kappa metrics to measure prediction agreement with true ground truth labels. The  work aims to find out which methodology offers the best performance-reliability tradeoff, especially in clinical diagnostic scenarios.

### Data Information
- Dataset source - https://figshare.com/articles/dataset/brain_tumor_dataset/1512427
- Documentation - https://figshare.com/articles/dataset/brain_tumor_dataset/1512427?file=51340418
- Data Type - Matlab Images
- Classes - has classes like **glioma**, **meningioma** and **pituitary tumors**
- Who published the data -  The data were Authored and published by **Jun Cheng** [*dataset posted on 2024-12-21, 15:57 authored by Jun Cheng*]
- When -  First published online on **2024-12-21 , 15:57**
"""



import os, glob, random, cv2
import zipfile
import shutil
import warnings

import pandas as pd
import numpy as np

# plotting
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from PIL import Image, ImageFile
import seaborn as sns




# for reading mat files
from scipy.io import loadmat
from skimage.measure import regionprops, label as label_mask_ftn
import h5py
from tqdm import tqdm

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split

# modelling
import torch.optim as optim
import torch.nn as nn
import torch

from torchvision import datasets, models, transforms
from torch.utils.data import DataLoader




warnings.filterwarnings('ignore')
matplotlib.style.use('ggplot')
sns.set(style="whitegrid")

# For reproducibility
seed = 27
random.seed(seed)
os.environ['PYTHONHASHSEED'] = str(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)

# download the data from fig
!wget https://figshare.com/ndownloader/articles/1512427/versions/8



# delete and recreate the ./process directory in order to avoid overwriting
if os.path.exists("./process"):
    shutil.rmtree("./process")
os.makedirs("./process", exist_ok=True)
print("Recreated './process' folder.")

# unzip the donloaded file into ./process
if os.path.exists("8") and zipfile.is_zipfile("8"):
    with zipfile.ZipFile("8", 'r') as zip_ref:
        zip_ref.extractall("./process")
    print("Unzipped '8' into './process'.")
else:
    print("File '8' not found or is not a ZIP file.")

os.listdir("./process")

!cat "./process/README 2024.txt"

# lets get the target zips we want
ZIP_FILE_NAMES = [
    'brainTumorDataPublic_2299-3064.zip',
    'brainTumorDataPublic_767-1532.zip',
    'brainTumorDataPublic_1533-2298.zip',
    'brainTumorDataPublic_1-766.zip'
]
ZIP_FILE_NAMES

# recreate ./process/mat to store unzipped mat images
mat_dir = "./process/mat"
if os.path.exists(mat_dir):
    shutil.rmtree(mat_dir)
os.makedirs(mat_dir, exist_ok=True)
print("Recreated './process/mat' folder.")

for zip_path in ZIP_FILE_NAMES:
    full_zip_path = os.path.join("./process", zip_path)
    with zipfile.ZipFile(full_zip_path, 'r') as zip_ref:
        zip_ref.extractall(mat_dir)
    print(f"Extracted {zip_path} into './process/mat'.")

# lets get the file extracted and know they are how many
mat_files = glob.glob("./process/mat/**/*.mat", recursive=True)


print(f"Total .mat iamge files in './process/mat': {len(mat_files)}")

random_file = random.choice(mat_files)

with h5py.File(random_file, 'r') as f:
    print(f"Keys in {random_file}:")
    print(list(f.keys()))

    #data
    cjdata = f['cjdata']

    label_data = cjdata['label']
    label = int(label_data[()][0][0])
    print("label is", label)

    image_data = cjdata['image']
    image_np = image_data[()]

    print("image shape is", image_np.shape)

    mask = cjdata['tumorMask'][()]

    print("Mask Shape is ", mask.shape)

#  a function to process the files
# The processing is an extract from the readme file above
def process_mat_files(input_path, output_path="./processed_data"):
    """
    convert .mat files to image and mask files, and returns a df
    with columns: ['image_path', 'mask_path', 'label']
    """
    os.makedirs(output_path, exist_ok=True)
    image_path = os.path.join(output_path, "images")
    mask_path = os.path.join(output_path, "masks")
    os.makedirs(image_path, exist_ok=True)
    os.makedirs(mask_path, exist_ok=True)

    mat_files = glob.glob(os.path.join(input_path, "*.mat"))
    records = []

    for mat_file in tqdm(mat_files, desc="Processing .mat files"):
        try:
            with h5py.File(mat_file, 'r') as f:
                cjdata = f['cjdata']

                # get the image and convert to uint8
                image = cjdata['image'][()]
                image = (255 * (image - np.min(image)) / (np.max(image) - np.min(image))).astype(np.uint8)

                # get the mask for tumor
                #we are doing just classification, the mask will be for visualization part
                mask = cjdata['tumorMask'][()].astype(np.uint8) * 255

                # label
                label = int(cjdata['label'][()][0][0])

                # create filenames
                base_name = os.path.splitext(os.path.basename(mat_file))[0]
                img_file_path = os.path.join(image_path, f"{base_name}.png")
                mask_file_path = os.path.join(mask_path, f"{base_name}_mask.png")

                # save them
                Image.fromarray(image).save(img_file_path)
                Image.fromarray(mask).save(mask_file_path)

                # keep record
                records.append({
                    'image_path': img_file_path,
                    'mask_path': mask_file_path,
                    'label': label
                })
        except Exception as e:
            print(f"Failed to process {mat_file}: {e}")
    df = pd.DataFrame(records)
    return df

df = process_mat_files(input_path="./process/mat")

df.head()

# rename 1 for meningioma, 2 for glioma, 3 for pituitary tumor

label_mapping_to_name = {
    1: "meningioma",
    2: "glioma",
    3: "pituitary tumor"
}

# create a new column for label names
df["label_name"] = df["label"].map(label_mapping_to_name)
df.head()



# lets check the distribution of labels
labels, lbl_counts = np.unique(df["label"], return_counts=True)


# the labels and also color
label_names = ['Meningioma', 'Glioma', 'Pituitary Tumor']
colors = ['#0072B2', '#D55E00', '#F0E442']

# plot piue chart
plt.figure(figsize=(8, 8))
plt.pie(lbl_counts,
        labels=label_names,
        colors=colors,
        autopct='%1.1f%%',
        startangle=90, textprops={'fontsize': 12})
plt.title("Brain Tumor Type Distribution", fontsize=16, weight='bold')
plt.axis('equal')
plt.show()



# sample out some random imgs
sample_df = df.sample(n=18, random_state=42).reset_index(drop=True)

plt.figure(figsize=(16, 8))
for i in range(18):
    image_path = sample_df.loc[i, 'image_path']
    mask_path = sample_df.loc[i, 'mask_path']
    label = str(sample_df.loc[i, 'label_name'])
    #read the images
    image = np.array(Image.open(image_path))
    mask = np.array(Image.open(mask_path)) > 0 #supposed to be boolena

    # do a plot
    plt.subplot(3, 6, i + 1)
    plt.imshow(image, cmap='bone')

    # make black in mask as transparent--Nothing
    masked = np.ma.masked_where(mask == False, mask)
    plt.imshow(masked, alpha=0.3, cmap='autumn')

    plt.title(label, fontsize=10)
    plt.axis('off')

plt.suptitle("Brain Tumor Samples with Masks", fontsize=16, fontweight="bold")
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()



"""---
# Exploratory Analysis of Brain Tumor Images
The main goal for the anasysis is to be able to identify whether there are distinguishable characteristics that can be used to differentiate the tumors. It will main be quantifying the most relevant features in images
We are going to extract and analyze a set of morphological and intensity-based features from the tumor regions. This will help in informations like tumor size, shape, intersity, patterns etcs of the 3 tumor types we have.


## Features Extracted and Their Relevance
- Tumor Size (`tumor_size`) -  The total number of pixels that the tumor occupies in the mask. It represents tumor volumnn in 2d slice which can be used to show tumor growth and how severe it is

- Tumor Area (`tumor_area`) - It is calculated  by the region properties function, similar to tumor size, which is the area of the contiguous tumor area.

- Tumor Perimeter (`tumor_perimeter`) - The border perimeter of the tumor. It helps in accessing tumor complexity and irregularity i.e those tumor which are more irregular (on boarders) have larger perimeter.

- Eccentricity (`eccentricity`) - Measures how elongated the tumor shape is, with 0 being a perfect circle and values approaching 1 indicating more elongated shapes. Help recognize tumors based on shape.

- Solidity (`solidity`) - Ratio between the area of the tumor and the area of the convex hull of the tumor. It defines how "compact" or "filled" the tumor area is, indicating shape irregularities such as indentations or concavities.

- Aspect Ratio (`aspect_ratio`) - This is the ratio of the width to the height of the tumor bounding box. It shows shape elongation of the tumor

- Mean Intensity (`mean_intensity`) - The average pixel intensity of the tumor region. SHows how bright/contrast the tumor tissues are in the MRI slice.

- Standard Deviation of Intensity (`std_intensity`) - It shows how variable the pixel intensity within the tumor.  The higher the value, the more hetegorogenous the tumor is.

Through extraction of these features on a representative subsample of the dataset (we sample in order to get the representation of the images population as well as utilize available resouses well by using less resources for extraction), It helps provide more detailed analysis and visualization of statistical comparison that will identify characteristic patterns between tumor types.
It is necessary to provide a foundation for later stages, including classification modeling and clinical interpretation.

---
"""

tumor_stats = []

# we will just use about 1k images
SAMPLE_SIZE = 1000
sample_df = df.sample(SAMPLE_SIZE, random_state=2025)
# iterate through the images
for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):
    image = np.array(Image.open(row['image_path']))
    mask = np.array(Image.open(row['mask_path'])) > 0
    if mask.sum() == 0:
        continue  # skip empty masks

    # get region properties
    labeled_mask = label_mask_ftn(mask)
    props = regionprops(labeled_mask)

    if len(props) == 0:
        continue

    #I'm assuming no image has more than 1 tumor.
    #  In this case we just analysise one tumor per image
    region = props[0]

    minr, minc, maxr, maxc = region.bbox
    width = maxc - minc
    height = maxr - minr
    aspect_ratio = width / height if height > 0 else 0


    tumor_stats.append({
        'label': row['label'],
        'label_name': row['label_name'],
        'tumor_size': mask.sum(),
        'tumor_area': region.area,
        'tumor_perimeter': region.perimeter,
        'eccentricity': region.eccentricity,
        'mean_intensity': image[mask].mean(),
        'std_intensity': image[mask].std(),
        'centroid_y': region.centroid[0],
        'centroid_x': region.centroid[1],
        'solidity': region.solidity,
        'aspect_ratio': aspect_ratio,

    })
stat_df = pd.DataFrame(tumor_stats)

stat_df.head()

stat_df.describe().T

"""# I. Dimensionality Reduction using PCA
In this case, through the use of PCA, we are trying to check if the flattened features can vusually split the different tumor types
"""

# get a sample data and run pca and plot
X = np.stack([np.array(Image.open(p).resize((128, 128))).flatten() for p in sample_df.image_path])
y = sample_df.label.values

# create 2d so that we can easily plot
X_pca = PCA(n_components=2).fit_transform(X)

# plot
plt.figure(figsize=(8, 8))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=sample_df.label_name, palette="Set1")
plt.title("PCA of Brain Tumor Images", fontweight="bold")
plt.show()

"""As it can be observed `pituitary tumor` seems to be very distint from the other two. It also have extreme values (this is based on principal values)

### II. Tumor Size by Class
- This helps shows which tumor appears bigger/has more pixels on average than the other
"""

# plot a box plot for this
plt.figure(figsize=(10, 6))
sns.boxplot(data=stat_df, x='label_name', y='tumor_size', palette='Set2')
plt.title('Tumor Size Distribution by Class')
plt.ylabel('Tumor Size (pixels)')
plt.xlabel('Tumor Type')
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

"""### Mean Intensity Inside Tumor"""

plt.figure(figsize=(10, 6))
sns.violinplot(data=stat_df, x='label_name', y='mean_intensity', palette='Set3')
plt.title('Mean Intensity Inside Tumor Region')
plt.ylabel('Mean Intensity')
plt.xlabel('Tumor Type')
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()



"""### III.  Tumor Eccentricity Comparison
- shows how elongated the tumor is (0 = circle, 1 = line)
"""

plt.figure(figsize=(10, 6))
sns.violinplot(data=stat_df, x='label_name', y='eccentricity', palette='Set2')
plt.title('Tumor Eccentricity by Class')
plt.ylabel('Eccentricity (0 = round, 1 = line-like)')
plt.xlabel('Tumor Type')
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()



"""### IV. Tumor Perimeter Comparison
- Shows how the length of tumor boundary is compaired with others
"""

plt.figure(figsize=(10, 6))
sns.boxplot(data=stat_df, x='label_name', y='tumor_perimeter', palette='Pastel1')
plt.title('Tumor Perimeter by Class')
plt.ylabel('Perimeter (pixels)')
plt.xlabel('Tumor Type')
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

"""### V. Tumor Aspect Ratio Comparison
- Its Width/Height of bounding box measurement
"""

plt.figure(figsize=(10, 6))
sns.boxplot(data=stat_df, x='label_name', y='aspect_ratio', palette='Set3')
plt.title('Tumor Aspect Ratio (Width / Height) by Class')
plt.ylabel('Aspect Ratio')
plt.xlabel('Tumor Type')
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()



"""### VI. Solidity Comparison (shape fill-ness)
Shows how the Area / Convex Hull Area or how filled the shape is
"""

plt.figure(figsize=(10, 6))
sns.boxplot(data=stat_df, x='label_name', y='solidity', palette='coolwarm')
plt.title('Tumor Solidity by Class')
plt.ylabel('Solidity (area / convex area)')
plt.xlabel('Tumor Type')
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()



"""### VII. Pixel Intensity Histograms by Class

"""

# plot intensity distribution for the masks
plt.figure(figsize=(12, 6))
for label in [1, 2, 3]:
    class_images = sample_df[sample_df.label == label]
    pixels = []
    for _, row in class_images.iterrows():
        img = np.array(Image.open(row["image_path"]))
        mask = np.array(Image.open(row["mask_path"])) > 0
        pixels.extend(img[mask])

    sns.histplot(pixels, label=label_mapping_to_name[label], kde=True, stat='density', bins=50)

plt.title("Pixel Intensity Distributions Within Tumor Regions")
plt.xlabel("Pixel Intensity")
plt.ylabel("Density")
plt.legend()
plt.tight_layout()
plt.show()





"""### VIII. Tumor Location Heatmap
This shows where tumors tend to appear in image space.
"""

# get a size to resizes our images to
target_size = (256, 256)
heatmap = np.zeros(target_size, dtype=np.float32)
for _, row in sample_df.iterrows():
    mask = Image.open(row['mask_path']).convert('L').resize(target_size)
    mask = np.array(mask) > 0
    heatmap += mask.astype(np.float32)

plt.figure(figsize=(16, 16))
plt.imshow(heatmap, cmap='hot')
plt.title(f"Tumor Location Heatmap (Resized to {target_size})", fontweight="bold", fontsize=17)
plt.colorbar(label='Tumor Occurrence Frequency')
plt.xlabel("X (Pixels)")
plt.ylabel("Y (Pixels)")
plt.grid(False)
plt.show()





"""---
#### Data Splitting Methodology
The data is going to be divided into three subsets which does not overlap which are:
- Training subset (60%) - Which will be the main split used for training the model
- validation subset (20%) - For tuning model hyperparameters and overfitting checking. This is used during the training process/validates the model
- Test Set (20%) - For final model testing and evaluations. This is what we will used to get perfomance metrics

The split will be performed in two phases with **stratified sampling** to maintain the **class distributions** (meningioma, glioma, pituitary tumor) across all subsets as folows;
1. **First Split** -  60% of data was allocated to the training set, and the other 40% was kept temporarily separate.
2. **Second Split** - This is a temporary split which is divided equally into the validation and test sets (each 20% of the total data).
This approach ensures the model to be trained and tested on various data and finally tested on **fully unseen** instances, which is critical in measuring real-world generalization.
"""

df.head()

# perform the first split 60:40
train_df, temp_df = train_test_split(
    df,
    test_size=0.4,
    stratify=df.label,
    random_state=2025
)

# get the 40% split  into 20% val and 20% test
val_df, test_df = train_test_split(
    temp_df,
    test_size=0.5,
    stratify=temp_df.label,
    random_state=2025
)


print(
    f"""
Train Size {train_df.shape}  --> {len(train_df)/len(df):.4f}
Test Size {test_df.shape} --> {len(test_df)/len(df):.4f}
Val Size {val_df.shape} -->  {len(val_df)/len(df):.4f}
    """
)

